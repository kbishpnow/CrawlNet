# Web Crawler Example Using CrawlNet.Crawler

This project demonstrates how to use `CrawlNet.Crawler` to fetch and extract website links while respecting `robots.txt` rules. The crawler checks if a website allows crawling before extracting its links.

## Features
- Reads `robots.txt` to determine if crawling is allowed.
- Extracts all valid HTTP/HTTPS links from a webpage.
- Prints the extracted links to the console.

